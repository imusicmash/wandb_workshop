{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imusicmash/wandb_workshop/blob/main/Copy_of_prompt_engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2OHNIWnnoiA"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wandb/llm-workshop-fc2024/blob/main/part_1_prompting/prompt_engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "<!--- @wandbcode{llm-workshop-fc2024-prompting} -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY48sjOgnoiC"
      },
      "source": [
        "# Prompting Workshop with Weights and Biases - [Anish Shah](https://www.linkedin.com/in/anish-shah/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkukrJ2UjfZt"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install set-env-colab-kaggle-dotenv -q\n",
        "!pip install weave -U -q\n",
        "!pip install litellm -U -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6-SApa0noiF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db99c6cc-a246-4ed8-a9b6-834873dbbac4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llm-workshop-fc2024'...\n",
            "remote: Enumerating objects: 160, done.\u001b[K\n",
            "remote: Counting objects: 100% (160/160), done.\u001b[K\n",
            "remote: Compressing objects: 100% (116/116), done.\u001b[K\n",
            "remote: Total 160 (delta 90), reused 97 (delta 37), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (160/160), 727.72 KiB | 12.13 MiB/s, done.\n",
            "Resolving deltas: 100% (90/90), done.\n",
            "/content/llm-workshop-fc2024/part_1_prompting\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    !git clone https://github.com/wandb/llm-workshop-fc2024.git\n",
        "    %cd llm-workshop-fc2024/part_1_prompting\n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hu7OtFu3jekl"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "from text_formatting import render\n",
        "from set_env import set_env\n",
        "set_env(\"ANTHROPIC_API_KEY\")\n",
        "set_env(\"WANDB_API_KEY\")\n",
        "set_env(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e8WkuQrkGvY"
      },
      "source": [
        "Welcome to the Prompting Workshop with Weights and Biases, led by Anish Shah. This workshop is designed to explore the fascinating world of prompt engineering, a crucial aspect of interacting with and leveraging the capabilities of large language models (LLMs). Throughout this session, we'll dive into various techniques for crafting effective prompts that can significantly enhance the performance of LLMs across a wide range of tasks.\n",
        "\n",
        "Whether you're new to AI and machine learning or looking to deepen your understanding of prompt engineering, this workshop will provide you with valuable insights and practical skills. By the end of this session, you'll be equipped to design and implement prompts that effectively communicate your intentions to LLMs, enabling more accurate and relevant responses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7A2mmTzYnoiI"
      },
      "source": [
        "This section of the notebook focuses on setting up the environment and installing the required libraries:\n",
        "\n",
        "- It installs the [weave](https://wandb.github.io/weave/) which is used for tracking llm model operations\n",
        "- It installs the `litellm` which is used to standardize model interaction and also make it easy to swap model providers\n",
        "- Some accessory functions are provided for better rendering and environment variable setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62fUUcwNnoiK"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import weave\n",
        "from litellm import completion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNZgBVTenoiM"
      },
      "source": [
        "## Model and Prompt Configuration\n",
        "The code snippets define important configuration variables for the prompting workshop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_b_UClQnoiN"
      },
      "outputs": [],
      "source": [
        "# These variables store the names of different language models from Anthropic and OpenAI.\n",
        "# The \"SMART\" models (`claude-3-opus` and `gpt-4-turbo`) are more capable but slower,\n",
        "# while the \"FAST\" models (`claude-3-haiku` and `gpt-3.5-turbo`) are faster but less powerful.\n",
        "ANTHROPIC_SMART_MODEL_NAME = \"claude-3-opus-20240229\"\n",
        "ANTHROPIC_FAST_MODEL_NAME = \"claude-3-haiku-20240307\"\n",
        "OPENAI_SMART_MODEL_NAME = \"gpt-4-turbo-2024-04-09\"\n",
        "OPENAI_FAST_MODEL_NAME = \"gpt-3.5-turbo\"\n",
        "\n",
        "# These variables point to two different markdown files containing prompt engineering guides.\n",
        "# `AMAN_PROMPT_GUIDE` refers to Aman Chadha's guide, while `LILIAN_PROMPT_GUIDE` refers to Lilian Weng's guide.\n",
        "AMAN_PROMPT_GUIDE = \"aman_prompt_engineering.md\"\n",
        "LILIAN_PROMPT_GUIDE = \"lilianweng_prompt_engineering.md\"\n",
        "\n",
        "# Here, the `MODEL_NAME` variable is set to use Anthropic's fast model (`claude-3-haiku`),\n",
        "# and the `PROMPT_GUIDE` variable selects Lilian Weng's prompt engineering guide.\n",
        "MODEL_NAME = ANTHROPIC_SMART_MODEL_NAME\n",
        "PROMPT_GUIDE = LILIAN_PROMPT_GUIDE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCgL1iCfnoiP"
      },
      "source": [
        "These configuration variables allow workshop participants to easily switch between different models and prompt guides throughout the workshop by modifying the assigned values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9CO0m-6noiP"
      },
      "source": [
        "## Initializing Weave\n",
        "\n",
        "This line initializes the Weave library for the \"prompting-workshop\" project. Weave is a toolkit for developing Generative AI applications, providing features like logging, debugging, evaluations, and organization of LLM workflows.\n",
        "\n",
        "Initializing Weave at the start allows you to leverage its capabilities throughout your project, such as decorating Python functions with `@weave.op()` to enable automatic tracing and versioning.\n",
        "\n",
        "By specifying the project name \"prompting-workshop\", you are setting up a dedicated workspace for this workshop within Weave. This helps keep the workshop-related experiments, models, and data organized and separate from other projects.\n",
        "\n",
        "Weave brings structure and best practices to the experimental nature of Generative AI development, making it easier to track, reproduce, and share your work. Initializing it early in the notebook ensures you can take full advantage of its features as you progress through the workshop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzV38KVSby6D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0402427-2c5f-40cd-f7d2-ba6efda25570"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logged in as W&B user alsmail10.\n",
            "View Weave data at https://wandb.ai/alsmail10/prompting-workshop-test/weave\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "weave.init(\"prompting-workshop-test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOMGik1DnoiQ"
      },
      "source": [
        "## Defining the get_completion function\n",
        "\n",
        "This code defines a function called `get_completion` that is decorated with `@weave.op()`. The `@weave.op()` decorator is provided by Weave and enables automatic tracing and versioning of the function.\n",
        "\n",
        "The `get_completion` function takes several parameters:\n",
        "- `system_message`: The system message to provide context or instructions to the language model.\n",
        "- `messages`: A list of messages representing the conversation history.\n",
        "- `model_name`: The name of the language model to use (defaults to `MODEL_NAME`).\n",
        "- `max_tokens`: The maximum number of tokens to generate in the response (defaults to 4096).\n",
        "- `temperature`: The sampling temperature for controlling the randomness of the generated text (defaults to 0).\n",
        "\n",
        "Inside the function, it calls the `completion` function (from the `litellm` library) with the provided parameters to generate a completion from the language model. The `temperature` parameter is set to 0, which is recommended for evaluations and RAG (Retrieval-Augmented Generation) systems to ensure deterministic results.\n",
        "\n",
        "The generated response is then printed as JSON using `response.json()`, and the JSON response is returned by the function.\n",
        "\n",
        "By using the `@weave.op()` decorator, Weave automatically tracks and versions the inputs and outputs of the `get_completion` function, making it easier to reproduce and analyze the results later in the workshop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfuTRXK0b1vm"
      },
      "outputs": [],
      "source": [
        "@weave.op()\n",
        "def get_completion(system_message: str, messages: list, model: str, max_tokens: int = 4096, temperature: float = 0, **kwargs) -> dict:\n",
        "    \"\"\"\n",
        "    Generates a completion using the specified model, taking into account the system message, conversation history, and additional arguments.\n",
        "\n",
        "    Parameters:\n",
        "        system_message (str): A message providing context or instructions for the model.\n",
        "        messages (list): A list of dictionaries representing the conversation history, where each dictionary has keys 'role' and 'content'.\n",
        "        model (str): The identifier of the model to use for generating completions.\n",
        "        max_tokens (int, optional): The maximum number of tokens to generate in the completion. Defaults to 4096.\n",
        "        temperature (float, optional): The sampling temperature to control the randomness of the generated text. Defaults to 0.\n",
        "        **kwargs: A dictionary of additional keyword arguments. Expected keys include 'system_message', 'model', 'max_tokens', and 'temperature'.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary representing the generated completion as JSON.\n",
        "    \"\"\"\n",
        "    # Adjust messages format based on the model type\n",
        "    kwargs = kwargs.get(\"kwargs\", {})\n",
        "    if \"gpt\" in model.lower():\n",
        "        formatted_messages = [{\"role\": \"system\", \"content\": system_message}] + messages\n",
        "    else:\n",
        "        kwargs[\"system\"] = system_message  # For non-gpt models, use system_message directly in kwargs\n",
        "\n",
        "    # Common arguments for the completion function\n",
        "    completion_args = {\n",
        "        \"model\": model,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"messages\": formatted_messages if \"gpt\" in model.lower() else messages,\n",
        "        **kwargs\n",
        "    }\n",
        "\n",
        "    # Generate and return the completion\n",
        "    response = completion(**completion_args)\n",
        "    return response.json()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8INIqZkInoiS"
      },
      "source": [
        "## Use Case: Building a Prompting Assistant\n",
        "\n",
        "In this section, we explore the practical application of prompt engineering by building a bot that helps users understand prompting techniques and answers questions based on the provided information. This use case demonstrates the power of prompt engineering in creating helpful AI assistants that can make complex topics more accessible and engaging.\n",
        "\n",
        "By leveraging the knowledge contained in a comprehensive guide on prompting techniques, we can develop a bot that provides accurate and relevant answers to user queries. Through careful crafting of system messages and prompt templates, we ensure that the bot's responses are not only informative but also easy to understand, even for beginners.\n",
        "\n",
        "Throughout this use case, workshop participants will learn how to:\n",
        "\n",
        "- Incorporate context to improve the relevance and accuracy of the model's responses\n",
        "- Use system messages to guide the model's behavior and output style\n",
        "- Standardize inputs and outputs for consistent and reusable prompting assistants\n",
        "- Experiment with different configurations to optimize the bot's performance\n",
        "\n",
        "By engaging with this use case, participants will gain hands-on experience in applying prompt engineering techniques to build a practical and helpful AI assistant. They will develop a deeper understanding of how to effectively communicate with language models and tailor their outputs to specific audiences and use cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BltDm9RhnoiT"
      },
      "source": [
        "### Step 1: Raw Prompting\n",
        "\n",
        "We start by sending a question to the language model without any additional context, using a basic `prompt_llm` function. This demonstrates the model's limitations when lacking the necessary information to provide relevant answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkDZPsohnoiT"
      },
      "outputs": [],
      "source": [
        "@weave.op()\n",
        "def prompt_llm(question: str, **kwargs) -> str:\n",
        "    \"\"\"\n",
        "    Sends a question to the language model and returns its response.\n",
        "\n",
        "    This function prepares a message with the user's question, handles additional\n",
        "    arguments for the language model, and invokes the get_completion function to\n",
        "    obtain a response. The response's content is then returned.\n",
        "\n",
        "    Parameters:\n",
        "        question (str): The question intended for the language model.\n",
        "        **kwargs: A dictionary of additional keyword arguments. Expected keys include 'system_message', 'model', 'max_tokens', and 'temperature'.\n",
        "\n",
        "    Returns:\n",
        "        str: The language model's response to the question.\n",
        "    \"\"\"\n",
        "    # Prepare the user's question for the language model\n",
        "    kwargs = kwargs.get(\"kwargs\", {})\n",
        "    messages = [{\"role\": \"user\", \"content\": question}]\n",
        "\n",
        "    # Extract additional parameters, applying defaults if necessary\n",
        "    system_message = kwargs.pop('system_message', \"\")\n",
        "    model = kwargs.pop('model', MODEL_NAME)\n",
        "    max_tokens = kwargs.pop('max_tokens', 4096)\n",
        "    temperature = kwargs.pop('temperature', 0)\n",
        "\n",
        "    # Compile arguments for the completion request\n",
        "    completion_args = {\n",
        "        \"system_message\": system_message,\n",
        "        \"messages\": messages,\n",
        "        \"model\": model,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature\n",
        "    }\n",
        "    completion_args.update(kwargs)  # Properly include any other additional arguments\n",
        "\n",
        "    # Request a completion from the language model\n",
        "    response = get_completion(**completion_args)\n",
        "\n",
        "    # Extract and return the content of the model's response\n",
        "    return response[\"choices\"][0][\"message\"][\"content\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRo5JZuXnoiU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06e6f62a-26fa-43ee-8a20-915e3d416ee7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/alsmail10/prompting-workshop-test/r/call/8e110eca-c54e-4a9c-8182-e5b02b2b597f\n"
          ]
        }
      ],
      "source": [
        "raw_prompt_response = prompt_llm(\n",
        "    \"Explain the latest prompting techniques and provide an example of each\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91xKdHRKnoiV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "d74dc210-cabe-4b99-b607-d8a3dbcd7201"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "I do not actually have knowledge about the latest prompting techniques. I am an AI assistant created\n",
              "by Anthropic to be helpful, harmless, and honest. I do not keep up with the latest developments in\n",
              "AI prompting methods."
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "render(raw_prompt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYcEVAjVnoiV"
      },
      "source": [
        "The model's response to the raw prompt is inadequate because it lacks the necessary context to provide a meaningful answer. Without any background information or specific details about prompting techniques, the model can only generate a generic, high-level response that fails to address the question effectively, many times providing no response at all.\n",
        "\n",
        "This poor performance highlights the importance of providing relevant context when prompting language models. By supplying the model with additional information related to the topic at hand, we can guide it towards generating more accurate, detailed, and useful responses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZ8iVhAgnoiW"
      },
      "source": [
        "### Step 2. Prompting with Context\n",
        "\n",
        "We can provide the necessary context to the language model by including it directly alongside the question. In this example, we use a comprehensive guide on prompting techniques written by [Lilian Weng](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/). This guide is particularly useful as it condenses many great papers and articles into a single-page resource, covering various prompting techniques.\n",
        "\n",
        "To incorporate the context, we:\n",
        "\n",
        "1. Load the markdown file containing the prompting guide using the `load_markdown_file` function.\n",
        "2. Concatenate the loaded context with the question in the `context_prompt_response` variable.\n",
        "3. Pass the combined context and question to the `prompt_llm` function to generate a response.\n",
        "\n",
        "By providing the model with relevant context, we expect to receive more accurate and informative answers to our questions about prompting techniques.\n",
        "\n",
        "Note: As an alternative, you can also use a more extensive guide by [Aman Chadha](https://aman.ai/primers/ai/prompt-engineering/) for additional context and information on prompt engineering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOOOjtk_noiW"
      },
      "outputs": [],
      "source": [
        "def load_markdown_file(file_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Reads and returns the content of a markdown file specified by its path.\n",
        "\n",
        "    Parameters:\n",
        "        file_path (str): The path to the markdown file to be read.\n",
        "\n",
        "    Returns:\n",
        "        str: The content of the markdown file as a string.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        markdown_content = file.read()\n",
        "    return markdown_content\n",
        "context = load_markdown_file(PROMPT_GUIDE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCRNuO5WnoiX"
      },
      "source": [
        "Note: Anthropic has an amazingly large context size and as a result we can luckily just shove the whole document into the prompt in this situation. This can get quite expensive however so it typically makes more sense to use techniques that chunk the document into better sizes or use a RAG based pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aP8SonqNnoiX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "549b7603-2553-45b2-db64-ea01a411c3af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/alsmail10/prompting-workshop-test/r/call/1c6866c3-f466-4b7f-affc-c922d966771f\n"
          ]
        }
      ],
      "source": [
        "context_prompt_response = prompt_llm(\n",
        "    context + \"\\n\\nExplain the latest prompting techniques and provide an example of each\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMoqCLOJnoiY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "outputId": "81cd9f59-ddab-4b0f-d542-0138bcead15a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Here are some of the latest prompting techniques along with examples of each:  1. Chain-of-Thought\n",
              "(CoT) Prompting: Generates a sequence of short sentences describing step-by-step reasoning to arrive\n",
              "at the final answer. Especially helpful for complex reasoning tasks.  Example: Question: Marty has\n",
              "100 centimeters of ribbon that he must cut into 4 equal parts. Each of the cut parts must be divided\n",
              "into 5 equal parts. How long will each final cut be?  Answer: Let's think step by step. - Marty has\n",
              "100 cm of ribbon total - He needs to cut it into 4 equal parts. So 100 cm / 4 = 25 cm per part after\n",
              "the first set of cuts.   - Then each 25 cm part needs to be cut into 5 equal pieces. - 25 cm / 5 = 5\n",
              "cm Therefore, each final cut piece will be 5 cm long.  2. Zero-Shot CoT: Uses a natural language\n",
              "prompt like \"Let's think step by step\" to encourage the model to generate reasoning chains before\n",
              "providing the final answer, without including few-shot examples.  Example:  Question: Jack is a\n",
              "soccer player. He needs to buy two pairs of socks and a pair of soccer shoes. Each pair of socks\n",
              "cost $9.50, and the shoes cost $92. Jack has $40. How much more money does Jack need? Answer: Let's\n",
              "think step by step to solve this problem.  3. Self-Consistency: Samples multiple outputs with\n",
              "temperature > 0 and selects the majority vote answer. Improves reasoning accuracy.  4. Complexity-\n",
              "Based Prompting: Uses prompts with demonstrations of higher reasoning complexity (more steps in the\n",
              "chain of thought). Improves performance on complex reasoning questions.  5. Automatic Prompt\n",
              "Generation: Methods like AutoPrompt search over a pool of model-generated instruction candidates to\n",
              "find the optimal prompt that maximizes a score function on a dataset.  6. Retrieval-Augmented\n",
              "Prompting: Retrieves relevant documents/paragraphs from a knowledge base and incorporates them into\n",
              "the prompt to provide additional context to the model. Helps with tasks requiring knowledge not\n",
              "captured during pretraining.  7. Program-Aided Prompting: Generates code (e.g. Python) within the\n",
              "prompt to offload complex computation to a runtime interpreter. Allows decoupling reasoning from\n",
              "computation.  8. Tool-Augmented Prompting: Augments the model with the ability to make API calls to\n",
              "external tools (e.g. calculator, search engine, calendar). Expands model's capabilities through tool\n",
              "use.  So in summary, the latest prompting techniques focus on eliciting step-by-step reasoning,\n",
              "improving consistency, automatically optimizing prompts, and augmenting language models with\n",
              "retrieval and tool use capabilities. The field is rapidly evolving to make LLMs more capable\n",
              "reasoners and problem solvers."
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "render(context_prompt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FhSZ_pynoiY"
      },
      "source": [
        "This response is a significant improvement! We can see that by providing the model with relevant context, it can generate an answer that includes details about various prompting techniques covered in the workshop. The model effectively utilizes the information from the provided guide to address the question more comprehensively.\n",
        "\n",
        "However, there is still room for improvement. The model tends to regurgitate the technical information from the guide without simplifying or explaining the concepts in an easily understandable manner. The response may be too complex or jargon-heavy for beginners or those new to the topic of prompt engineering.\n",
        "\n",
        "To address this issue, we need to guide the model towards providing explanations that are more accessible and beginner-friendly. This is where the next step of conditioning the model's responses with a carefully crafted system prompt comes into play. By instructing the model to break down the technical details and present the information in a more digestible format, we can ensure that the responses are not only informative but also easy to understand for a broader audience."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBtOob3ynoiZ"
      },
      "source": [
        "### Step 3. Condition Responses with a System Prompt\n",
        "\n",
        "To ensure that the bot explains the information in a way that is easy to understand, we can provide a system prompt that guides the model to present the content in a beginner-friendly manner. Here’s why this system prompt is effective:\n",
        "\n",
        "1. **Objective Clarity**: It directly states the task — simplifying prompt engineering concepts with examples. This aligns with the principle of having a clear and specific objective, which helps the LLM focus on the exact task ([source](https://medium.com/the-modern-scientist/best-prompt-techniques-for-best-llm-responses-24d2ff4f6bca)).\n",
        "\n",
        "2. **Tone Specification**: Setting a friendly and educational tone guides the LLM on the desired interaction style, making the information approachable and digestible.\n",
        "\n",
        "3. **Context Awareness**: By acknowledging the user's basic AI knowledge, the prompt tailors the complexity of the content, ensuring it is suitable for beginners without being overly simplistic.\n",
        "\n",
        "4. **Guidance on Style**: Instructing the use of analogies and simple examples helps in breaking down complex topics into understandable segments, which is crucial for teaching technical subjects effectively.\n",
        "\n",
        "5. **Verification of Output**: Emphasizing clarity and relevance ensures that the responses are not only correct but also useful and directly applicable to the user’s needs.\n",
        "\n",
        "6. **Highlighting Benefits**: Mentioning the benefits of simplifying technical concepts engages users by showing the value of what they are learning, enhancing their motivation and the educational impact.\n",
        "\n",
        "### General Approach to Constructing Effective System Prompts\n",
        "\n",
        "When constructing system prompts for LLMs, consider the following steps to ensure effectiveness and clarity:\n",
        "\n",
        "- **Define the Objective**: Clearly state what you want the LLM to achieve. This should be specific and concise.\n",
        "- **Set the Tone and Style**: Indicate how the response should feel or sound. This helps the LLM adjust its language and approach.\n",
        "- **Provide Necessary Context**: Include any background information that will help the LLM understand the scope and depth of the response required.\n",
        "- **Incorporate Guidance for Content**: Direct the LLM on how to structure its response or what elements to include, such as examples or analogies.\n",
        "- **Specify Output Format**: If necessary, define how the response should be formatted. This is particularly important for tasks requiring a specific output structure.\n",
        "- **Use Clear and Direct Language**: Avoid ambiguity by using straightforward and direct language. This reduces the chances of misinterpretation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSF_lq7XnoiZ"
      },
      "outputs": [],
      "source": [
        "system_message = \"\"\"\n",
        "Objective: Simplify prompt engineering concepts for easy understanding. Provide clear examples for each technique.\n",
        "Tone: Friendly and educational, suitable for beginners.\n",
        "Context: Assume basic AI knowledge; avoid deep technical jargon.\n",
        "Guidance: Use metaphors and simple examples to explain concepts. Keep explanations concise and applicable.\n",
        "Verification: Ensure clarity and relevance in responses, with practical examples.\n",
        "Benefits: Help users grasp prompt engineering basics, enhancing their AI interaction experience.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATpXbHLcnoia",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73d6a2e9-c886-4274-aa7b-c3b8590d6750"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/alsmail10/prompting-workshop-test/r/call/f491b5d0-a877-4f98-9a0a-6f14aa29fc82\n"
          ]
        }
      ],
      "source": [
        "system_and_context_prompt_response = prompt_llm(\n",
        "    system_message=system_message,\n",
        "    question=context + \"\\n\\nExplain the latest prompting techniques and provide an example of each\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXv8L38wnoia",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "524f8f2f-91f5-4ffd-9b72-def31b9e8c79"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Here is a summary of the latest prompting techniques with examples for each:  1. Zero-Shot Prompting\n",
              "Provide the task directly to the model without any examples.  Example: Text: i'll bet the video game\n",
              "is a lot more fun than the film.   Sentiment:  2. Few-Shot Prompting  Provide a few examples\n",
              "demonstrating the task before the actual input.  Example: Text: (lawrence bounces) all over the\n",
              "stage, dancing, running, sweating, mopping his face and generally displaying the wacky talent that\n",
              "brought him fame in the first place. Sentiment: positive  Text: despite all evidence to the\n",
              "contrary, this clunker has somehow managed to pose as an actual feature movie, the kind that charges\n",
              "full admission and gets hyped on tv and purports to amuse small children and ostensible adults.\n",
              "Sentiment: negative  Text: i'll bet the video game is a lot more fun than the film. Sentiment:  3.\n",
              "Instruction Prompting Provide detailed instructions describing the task requirements.  Example:\n",
              "Please label the sentiment towards the movie of the given movie review. The sentiment label should\n",
              "be \"positive\" or \"negative\". Text: i'll bet the video game is a lot more fun than the film.\n",
              "Sentiment:  4. Chain-of-Thought Prompting Encourage the model to break down its reasoning into a\n",
              "step-by-step process.  Example: Question: Marty has 100 centimeters of ribbon that he must cut into\n",
              "4 equal parts. Each of the cut parts must be divided into 5 equal parts. How long will each final\n",
              "cut be? Answer: Let's think step by step.  5. Self-Consistency Sampling Generate multiple outputs\n",
              "with sampling and select the majority vote answer.  6. Retrieval Augmentation  Retrieve relevant\n",
              "information from an external knowledge base and include it in the prompt.  7. Tool Use Allow the\n",
              "model to use external tools and APIs to augment its capabilities.  The key is to provide clear\n",
              "instructions, relevant examples, and encourage step-by-step reasoning to get the best performance\n",
              "out of language models via prompting. Techniques like retrieval and tool use further expand what\n",
              "prompting can accomplish."
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "render(system_and_context_prompt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyiiI18Enoia"
      },
      "source": [
        "Great! Now we're able to get a response that is easy to understand and provides a lot of context. The model has successfully broken down the technical concepts into beginner-friendly explanations, using simple language, analogies, and examples to convey the key points. This approach makes the information more accessible and engaging for those new to prompt engineering, fostering a deeper understanding of the subject matter.\n",
        "\n",
        "The next step is to standardize the inputs and outputs in a way that allows us to ask different questions and pass different context in the future. By creating a consistent structure for our prompts and responses, we can streamline the development of LLM applications and make it easier to experiment with various configurations. This standardization will enable us to quickly iterate on our prompts, test different contexts, and fine-tune our models to achieve the best possible results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sn9rPnEKnoib"
      },
      "source": [
        "### Step 4: System Prompts - Inputs\n",
        "\n",
        "To make it easier to experiment with different parameters and retrieve our best models, we can wrap our prompting logic in a collection of modular functions decorated with `@weave.op()`. This allows us to track and version our operations, making it easier to reproduce and analyze our results. We can also define a standard input structure for our system prompts, ensuring consistency across different prompts and use cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYrVnakqnoib"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"{context}\\n{question}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rVo5h05noic"
      },
      "outputs": [],
      "source": [
        "@weave.op()\n",
        "def format_prompt(prompt_template: str, **kwargs):\n",
        "    \"\"\"\n",
        "    Formats a prompt template with provided keyword arguments.\n",
        "\n",
        "    This function takes a template string and a dictionary of keyword arguments,\n",
        "    then formats the template string using these arguments.\n",
        "\n",
        "    Parameters:\n",
        "        prompt_template (str): The template string to be formatted.\n",
        "        **kwargs (dict): Keyword arguments to format the template string with.\n",
        "\n",
        "    Returns:\n",
        "        str: The formatted prompt template.\n",
        "    \"\"\"\n",
        "    kwargs = kwargs.get(\"kwargs\", {})\n",
        "    return prompt_template.format(**kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCsaMQs0noid"
      },
      "outputs": [],
      "source": [
        "# In this app we assume only a context and question are passed to the prompt template but that need not be true\n",
        "@weave.op()\n",
        "def llm_app(prompt_template: str, context: str, question: str, **kwargs):\n",
        "    \"\"\"\n",
        "    Generates a response using a formatted prompt based on a template, context, and question.\n",
        "\n",
        "    This function formats a given prompt template with the specified context and question, then\n",
        "    generates a response using the prompt_llm function with additional keyword arguments.\n",
        "\n",
        "    Parameters:\n",
        "        prompt_template (str): The template string used to format the prompt.\n",
        "        context (str): The context information to be included in the prompt.\n",
        "        question (str): The specific question to be asked in the prompt.\n",
        "        **kwargs (dict): Additional keyword arguments to be passed to the prompt_llm function.\n",
        "\n",
        "    Returns:\n",
        "        str: A string representing the generated response.\n",
        "    \"\"\"\n",
        "    kwargs = kwargs.get(\"kwargs\", {})\n",
        "    formatted_prompt = format_prompt(prompt_template=prompt_template, context=context, question=question)\n",
        "    response = prompt_llm(\n",
        "        question=formatted_prompt,\n",
        "        **kwargs\n",
        "    )\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f6_gG04noid"
      },
      "source": [
        "We defined our llm_app which in turns acts as the core of our prompting assistant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQcAmVxXnoid"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"\n",
        "Explain the differences between zero-shot, few-shot, and chain of thought\n",
        "prompting techniques? Please provide a clear explanation and a practical example\n",
        "for each technique within a structured format.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTCsSXzknoie",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b2059d5-0cfe-4737-cc07-a3d0bb8e0c02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/alsmail10/prompting-workshop-test/r/call/04e114a1-5487-4766-b4dd-34d45d52d116\n"
          ]
        }
      ],
      "source": [
        "input_template_response = llm_app(\n",
        "    system_message=system_message,\n",
        "    prompt_template=prompt_template,\n",
        "    context=context,\n",
        "    question=question,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeoRiMQ5noie",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "outputId": "3a0b4207-a2cd-4d8b-f838-ac4bf5eb5d45"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Here is a clear explanation of the differences between zero-shot, few-shot, and chain of thought\n",
              "prompting techniques, with a practical example for each:  Zero-Shot Prompting - Explanation: The\n",
              "task text is directly fed to the language model without any examples. The model tries to complete\n",
              "the task based solely on its pre-existing knowledge and capabilities. - Example:  Text: The movie\n",
              "was terrible. I hated every minute of it. Sentiment: <model_output>  Expected Output:  Sentiment:\n",
              "negative  Few-Shot Prompting   - Explanation: A small set of examples (usually 1-5) demonstrating\n",
              "the desired input/output format are provided to the model before the actual task. This helps prime\n",
              "the model on what kind of output is expected. - Example: Text: Best movie ever! Loved the acting and\n",
              "cinematography. Sentiment: positive  Text: The plot made no sense and the acting was atrocious.\n",
              "Sentiment: negative  Text: While it had some funny moments, overall the movie was just average.\n",
              "Sentiment: neutral  Text: I was on the edge of my seat the whole time. A real thriller!   Sentiment:\n",
              "<model_output>  Expected Output: Sentiment: positive  Chain of Thought (CoT) Prompting -\n",
              "Explanation: The model is prompted to break down its reasoning into a series of steps before\n",
              "providing the final answer. This can lead to more reliable and interpretable outputs, especially for\n",
              "complex reasoning tasks. - Example: Question: Michael had 58 golf balls. On tuesday, he lost 23 golf\n",
              "balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\n",
              "Answer: Let's solve this step by step: * Initially, Michael had 58 golf balls * On Tuesday, Michael\n",
              "lost 23 golf balls * So after Tuesday, Michael had 58 - 23 = 35 golf balls remaining * On Wednesday,\n",
              "Michael lost 2 more golf balls  * So after Wednesday, Michael had 35 - 2 = 33 golf balls remaining\n",
              "Therefore, at the end of Wednesday, Michael had 33 golf balls.  In summary: - Zero-shot uses no\n",
              "examples, relying only on the model's inherent knowledge  - Few-shot provides a small number of\n",
              "examples to demonstrate the task - Chain of thought breaks down the model's reasoning into\n",
              "interpretable steps  The key is to provide clear examples that highlight the unique aspects of each\n",
              "prompting technique. Let me know if you need any clarification or have additional questions!"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "render(input_template_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeMiVK15noif"
      },
      "source": [
        "Now we can easily and consistently swap system messages, context, and questions to get the best results. This allows us to test various combinations of system messages, context, and questions to find the most effective prompts for our specific use case.\n",
        "\n",
        "However, as we can see from the example, the current system prompt doesn't work as well with the newly asked question. This highlights the importance of tailoring the system prompt to the specific task at hand. Additionally, we may want to enforce more consistency in the format of our model's outputs. While using third-party packages like Instructor is beyond the scope of this workshop, we can achieve similar results by using proper tags in our prompt. By including specific tags or formatting instructions in the prompt, we can guide the model to respond in a way that is more consistent and easier to parse on our end\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3ClmP1Hnoig"
      },
      "source": [
        "### Step 5: System Prompts - Outputs\n",
        "\n",
        "In this step, we focus on improving the consistency and structure of our model's outputs by modifying the prompt template. By including specific tags and formatting instructions in the prompt, we can guide the model to respond in a way that is easier to parse and process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLzwQn77noig"
      },
      "source": [
        "In the system message, we've added a new tag called `Format` which provides instructions for the model to respond within an <answer></answer> tag, with separate <explanation> and <example> tags for each concept. This structured format helps organize the information and makes it easier to extract and analyze the responses programmatically.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seM8rOuUnoii"
      },
      "outputs": [],
      "source": [
        "def update_prompt_with_output_indicator(system_message: str, prompt_template: str):\n",
        "    if \"gpt\" in MODEL_NAME:\n",
        "        system_format_msg = \"\"\"\n",
        "    Format: Respond within a structured JSON object, using the keys provided in the prompt to organize your response.\n",
        "    Provide a condensed answer under the 'condensed_answer' key, detailed explanations under 'explanation' keys,\n",
        "    and examples under 'example' keys within each explanation.\n",
        "    \"\"\"\n",
        "        prompt_format_msg = \"\"\"\n",
        "    You must respond in JSON format.\n",
        "    Your response should follow this structure:\n",
        "    {{\n",
        "      \"answer\": {{\n",
        "        \"condensed_answer\": \"CONDENSED_ANSWER\",\n",
        "        \"explanation_1\": {{\n",
        "          \"detail\": \"EXPLANATION_1\",\n",
        "          \"example\": \"EXAMPLE_1\"\n",
        "        }},\n",
        "        \"explanation_2\": {{\n",
        "          \"detail\": \"EXPLANATION_2\",\n",
        "          \"example\": \"EXAMPLE_2\"\n",
        "        }},\n",
        "        ...\n",
        "      }}\n",
        "    }}\n",
        "    \"\"\"\n",
        "    else:\n",
        "        system_format_msg = \"\"\"\n",
        "    Format: Respond within an <answer></answer> tag, with as many <explanation></explanation> tags as needed,\n",
        "    ensuring that the <detail></detail> and <example></example> tags are used within each <explanation></explanation> tag.\n",
        "    Provide a condensed answer for the question in the <condensed_answer></condensed_answer> tag.\n",
        "    \"\"\"\n",
        "        prompt_format_msg = \"\"\"\n",
        "    You must respond within an <answer></answer> XML tags.\n",
        "    Inside of the <answer> markdown tag, you must provide a format of\n",
        "    <answer>\n",
        "        <condensed_answer> CONDENSED_ANSWER </condensed_answer>\n",
        "        <explanation>\n",
        "          <detail> EXPLANATION </detail>\n",
        "          <example> EXAMPLE </example>\n",
        "        </explanation>\n",
        "        <explanation>\n",
        "          <detail> EXPLANATION </detail>\n",
        "          <example> EXAMPLE </example>\n",
        "        </explanation>\n",
        "        ...\n",
        "    </answer>\n",
        "    \"\"\"\n",
        "    formatted_system_message = system_message + \"\\n\" + system_format_msg\n",
        "    formatted_prompt_template = prompt_template + \"\\n\" + prompt_format_msg\n",
        "\n",
        "    return formatted_system_message, formatted_prompt_template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGAdzPm3noij"
      },
      "outputs": [],
      "source": [
        "formatted_system_message, formatted_prompt_template = update_prompt_with_output_indicator(system_message, prompt_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xxeBejYnoij"
      },
      "source": [
        "Similarly, we've updated the `prompt_template` to include the new formatting instructions, ensuring that the model generates responses that adhere to the specified structure. By enforcing a consistent output format, we can streamline the processing of the model's responses and facilitate further analysis and evaluation of the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAPeoIZKnoik"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"\n",
        "Explain the differences between zero-shot, few-shot, and chain of thought\n",
        "prompting techniques? Please provide a clear explanation and a practical example\n",
        "for each technique within a structured format.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIIW9fimnoil",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5469f4a-012b-4763-cf5b-0557f8c87b8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/alsmail10/prompting-workshop-test/r/call/a7ad898a-1962-4a8f-a6dd-a7a008485406\n"
          ]
        }
      ],
      "source": [
        "output_indicator_response = llm_app(\n",
        "    system_message=formatted_system_message,\n",
        "    prompt_template=formatted_prompt_template,\n",
        "    context=context,\n",
        "    question=question,\n",
        "    # response_format={\"type\": \"json_object\"} # Comment this out for `Claude` models or `litellm.drop_params=True``\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiC9uC7Snoil",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "outputId": "9555c5a3-7b44-4993-eda2-da9786bdade8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Here is a structured explanation of the differences between zero-shot, few-shot, and chain of\n",
              "thought prompting techniques:  <answer> <condensed_answer> Zero-shot prompting provides the task to\n",
              "the model without any examples. Few-shot prompting includes a small number of input-output examples\n",
              "to demonstrate the task. Chain of thought prompting encourages the model to break down its reasoning\n",
              "into a series of steps. </condensed_answer>  <explanation> <detail> Zero-shot prompting simply\n",
              "provides the task or question to the language model without any examples of the desired output. The\n",
              "model must infer what is being asked and how to respond appropriately based solely on the prompt.\n",
              "</detail> <example> Zero-shot sentiment classification prompt: Text: The food was terrible and the\n",
              "service was even worse.  Sentiment: </example> </explanation>  <explanation> <detail> Few-shot\n",
              "prompting includes a small number (usually 1-5) examples of input-output pairs that demonstrate the\n",
              "desired task. By seeing a few examples, the model can better understand the required format and\n",
              "content of the responses. </detail> <example> Few-shot sentiment classification prompt: Text: The\n",
              "movie was fantastic! I loved every minute of it. Sentiment: Positive  Text: That was the worst\n",
              "restaurant experience I've ever had.   Sentiment: Negative  Text: The phone works well but the\n",
              "battery life is disappointing. Sentiment: </example> </explanation>  <explanation> <detail> Chain of\n",
              "thought prompting encourages the model to break down its reasoning process into a series of steps,\n",
              "explaining each part of the solution before providing the final answer. This can lead to more\n",
              "reliable and interpretable results, especially on complex reasoning tasks. </detail> <example> Chain\n",
              "of thought arithmetic prompt: Question: At the fair Adam bought 13 tickets. After riding the ferris\n",
              "wheel he had 4 tickets left. If each ticket cost 5 dollars, how much money did Adam spend on\n",
              "tickets?  Let's solve this step by step: - Adam started with 13 tickets  - He used 13 - 4 = 9\n",
              "tickets to ride the ferris wheel - Each ticket cost $5 - So for the 9 tickets he spent, the total\n",
              "cost was 9 * $5 = $45  Therefore, the total amount of money Adam spent on tickets was $45.\n",
              "</example> </explanation> </answer>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "render(output_indicator_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7NcT2fQnoim"
      },
      "source": [
        "## Advanced Prompting Techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GP5s-TJhnoin"
      },
      "source": [
        "### Zero-shot Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t71-Ifz0noin"
      },
      "outputs": [],
      "source": [
        "def update_with_zero_shot_prompt(question: str):\n",
        "    zero_shot_instruction = \"Without using any specific examples, \"\n",
        "    return zero_shot_instruction + question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jBN1cj1noin"
      },
      "outputs": [],
      "source": [
        "zero_shot_question = update_with_zero_shot_prompt(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fG5lcgg-noin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd473958-a77c-4e88-8404-54c3d7d62c74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/alsmail10/prompting-workshop-test/r/call/b1e3b221-860d-402f-870f-855593a30e1d\n"
          ]
        }
      ],
      "source": [
        "zero_shot_response = llm_app(\n",
        "    system_message=formatted_system_message,\n",
        "    prompt_template=formatted_prompt_template,\n",
        "    context=context,\n",
        "    question=zero_shot_question,\n",
        "    # response_format={\"type\": \"json_object\"} # Comment this out for `Claude` models or `litellm.drop_params=True``\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9z0Wn74snoio",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "9bfb301c-34bf-4a4a-e03b-e3929611375d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Here is a comparison of zero-shot, few-shot, and chain-of-thought prompting techniques: <answer>\n",
              "<condensed_answer> Zero-shot prompting provides the task to the model without any examples. Few-shot\n",
              "prompting includes a small number of input-output examples to demonstrate the task. Chain-of-thought\n",
              "prompting encourages the model to break down its reasoning into a series of steps.\n",
              "</condensed_answer>  <explanation> <detail> Zero-shot prompting simply provides the task or question\n",
              "to the language model without any examples of the desired output. The model must infer what is being\n",
              "asked and how to respond appropriately based solely on the prompt. </detail> <example> Prompt:\n",
              "Classify the sentiment of the following movie review as either positive or negative: \"This was the\n",
              "most boring, unoriginal film I've seen in years. I walked out of the theater feeling like I\n",
              "completely wasted my time and money.\" </example> </explanation>  <explanation> <detail> Few-shot\n",
              "prompting includes a small number (usually 1-5) examples of input-output pairs to demonstrate to the\n",
              "model what type of response is expected. This gives the model more context to understand the task.\n",
              "</detail> <example> Prompt:  Review: \"An instant classic! The acting was superb and the plot kept me\n",
              "on the edge of my seat the whole time.\" Sentiment: Positive  Review: \"Terrible acting, cliched\n",
              "dialogue, and a nonsensical plot. One of the worst movies of the year.\" Sentiment: Negative  Review:\n",
              "\"This was the most boring, unoriginal film I've seen in years. I walked out of the theater feeling\n",
              "like I completely wasted my time and money.\" Sentiment: </example> </explanation>  <explanation>\n",
              "<detail> Chain-of-thought prompting encourages the model to break down its reasoning process into a\n",
              "series of steps, explaining its \"thought process\" that leads to the final answer. This can lead to\n",
              "more reliable and interpretable outputs, especially on complex reasoning tasks. </detail> <example>\n",
              "Prompt: Marcus has 5 apples. Julia gives him 7 more apples. Marcus then gives 3 apples to his friend\n",
              "Alex. How many apples does Marcus have now? Let's solve this step-by-step: </example> </explanation>\n",
              "</answer>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "render(zero_shot_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onMW967Fnoip"
      },
      "source": [
        "### Few-shot Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GW0BvTv5noip"
      },
      "outputs": [],
      "source": [
        "def update_with_few_shot_prompt(question: str):\n",
        "    if \"gpt\" in MODEL_NAME:\n",
        "        few_shot_examples = \"\"\"\n",
        "        Here are a few examples of prompting techniques in JSON format:\n",
        "        {{\n",
        "            \"answer\": {{\n",
        "                \"condensed_answer\": \"Different prompting techniques are used to guide language models in generating desired outputs.\",\n",
        "                \"explanation_1\": {{\n",
        "                    \"detail\": \"Translation prompts provide the model with a source language text and request the translation in a target language.\",\n",
        "                    \"example\": \"Translate the following English text to French: 'Hello, how are you?'\"\n",
        "                }},\n",
        "                \"explanation_2\": {{\n",
        "                    \"detail\": \"Sentiment classification prompts ask the model to determine the sentiment expressed in a given text.\",\n",
        "                    \"example\": \"Classify the sentiment of the following text: 'The movie was terrible.'\"\n",
        "                }},\n",
        "                \"explanation_3\": {{\n",
        "                    \"detail\": \"Factual question prompts require the model to provide an answer along with an explanation or reasoning.\",\n",
        "                    \"example\": \"What is the capital of Germany? Explain your reasoning.\"\n",
        "                }}\n",
        "            }}\n",
        "        }}\n",
        "        \"\"\"\n",
        "    else:\n",
        "        few_shot_examples = \"\"\"\n",
        "        Here are a few examples of prompting techniques in XML format:\n",
        "        <answer>\n",
        "            <condensed_answer>Different prompting techniques are used to guide language models in generating desired outputs.</condensed_answer>\n",
        "            <explanation>\n",
        "                <detail>Translation prompts provide the model with a source language text and request the translation in a target language.</detail>\n",
        "                <example>Translate the following English text to Spanish: 'Good morning, how can I help you?'</example>\n",
        "            </explanation>\n",
        "            <explanation>\n",
        "                <detail>Sentiment classification prompts ask the model to determine the sentiment expressed in a given text.</detail>\n",
        "                <example>Classify the sentiment of the following text: 'I love this product!'</example>\n",
        "            </explanation>\n",
        "            <explanation>\n",
        "                <detail>Factual question prompts require the model to provide an answer along with an explanation or reasoning.</detail>\n",
        "                <example>What is the capital of Canada? Provide your thought process.</example>\n",
        "            </explanation>\n",
        "        </answer>\n",
        "        \"\"\"\n",
        "\n",
        "    return few_shot_examples + \"\\n\" + question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7Qi2td8noir"
      },
      "outputs": [],
      "source": [
        "few_shot_question = update_with_few_shot_prompt(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWF3lU1Cnoir",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "862df65a-51e1-4053-fdd2-cb6418503522"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/alsmail10/prompting-workshop-test/r/call/156143df-6b31-4e83-a4ca-3a4e16c64e62\n"
          ]
        }
      ],
      "source": [
        "few_shot_response = llm_app(\n",
        "    system_message=formatted_system_message,\n",
        "    prompt_template=formatted_prompt_template,\n",
        "    context=context,\n",
        "    question=few_shot_question,\n",
        "    # response_format={\"type\": \"json_object\"} # Comment this out for `Claude` models or `litellm.drop_params=True``\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGv_yqqBnoir",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "outputId": "09325062-c096-4c0e-bde7-c948476f37d7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Here is a comparison of zero-shot, few-shot, and chain of thought prompting techniques in the\n",
              "requested format:  <answer> <condensed_answer> Zero-shot uses no examples, few-shot uses a small\n",
              "number of examples, and chain of thought breaks down the reasoning process into steps, to guide\n",
              "language models in generating desired outputs. </condensed_answer>  <explanation> <detail> Zero-shot\n",
              "prompting provides the language model with only a task description or instruction, without any\n",
              "examples. The model must rely on its existing knowledge to generate a response. </detail> <example>\n",
              "Classify the sentiment of the following movie review text: \"The plot was full of holes and the\n",
              "characters were unbelievable. I wanted to walk out of the theater.\" </example> </explanation>\n",
              "<explanation> <detail> Few-shot prompting includes a small number of examples (typically 1-5) that\n",
              "demonstrate the desired input/output format or task. This helps prime the model to generate better\n",
              "responses. </detail> <example> Positive movie review: \"The cinematography was stunning and the\n",
              "acting was superb. I'd highly recommend this film!\" Negative movie review: \"The story made no sense\n",
              "and the dialogue was cringe-worthy. Save your money.\"  Classify the sentiment of the following\n",
              "review:   \"The plot was full of holes and the characters were unbelievable. I wanted to walk out of\n",
              "the theater.\" </example> </explanation>  <explanation> <detail> Chain of thought prompting has the\n",
              "model break down its reasoning process into a series of steps, which can lead to more reliable and\n",
              "interpretable outputs, especially for complex reasoning tasks. </detail> <example> What is 24 + 38?\n",
              "Show your work step-by-step.  Step 1: 24 + 30 = 54 Step 2: 54 + 8 = 62 Therefore, 24 + 38 = 62\n",
              "</example> </explanation> </answer>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "render(few_shot_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGNien5enois"
      },
      "source": [
        "### Chain of Thought"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjLzHlEenois"
      },
      "source": [
        "Note: We do not use the output indicators in this case as it will negate the chain of thought to instead enforce the formatting. It is important to explicitly incorporate the thought process desired in the prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2i8mCPWGnois"
      },
      "outputs": [],
      "source": [
        "def update_with_chain_of_thought_prompt(system_message: str, prompt_template: str):\n",
        "    chain_of_thought_instruction = \"Let's explicitly think step by step. My thought process is:\\n\"\n",
        "    chain_of_thought_system_format = \"Format: You must explicitly define the thought process and knowledge from the context to come to your conclusion for the question.\"\n",
        "    return system_message + \"\\n\" + chain_of_thought_system_format, prompt_template + \"\\n\" + chain_of_thought_instruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCixEhCNnoit"
      },
      "outputs": [],
      "source": [
        "chain_of_thought_system_message, chain_of_thought_prompt = update_with_chain_of_thought_prompt(system_message, prompt_template)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wG0q_rjnoiu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "265a17c2-6a6c-4f34-8ffd-fba02126e6f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/alsmail10/prompting-workshop-test/r/call/c373fbe1-df08-4f01-954b-42d2718c10ec\n"
          ]
        }
      ],
      "source": [
        "chain_of_thought_response = llm_app(\n",
        "    system_message=chain_of_thought_system_message,\n",
        "    prompt_template=chain_of_thought_prompt,\n",
        "    context=context,\n",
        "    question=question,\n",
        "    # response_format={\"type\": \"json_object\"} # Comment this out for `Claude` models or `litellm.drop_params=True``\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXAufUKTnoiu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "outputId": "90ca3d6b-d483-4374-f704-4bed8ca37bbf"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "To explain the differences between zero-shot, few-shot, and chain of thought prompting techniques, I\n",
              "will:  1. Define each technique clearly 2. Provide a practical example for each  3. Summarize the\n",
              "key differences in a structured format  Zero-Shot Prompting: Definition: Providing only the task\n",
              "input to the language model, with no examples of the desired output format. The model generates the\n",
              "output based solely on the input prompt.  Example:  Input: \"Classify the sentiment of the following\n",
              "movie review: I thought the movie was entertaining and fun to watch.\" Output: \"Positive\"  Few-Shot\n",
              "Prompting:  Definition: Providing a small number of input-output examples (usually 1-5)\n",
              "demonstrating the desired task, before giving the actual input prompt. The model learns from the\n",
              "examples to generate the output in the same format.  Example: Input:  \"Classify the sentiment of the\n",
              "following movie reviews:  Review: The acting was terrible and the plot made no sense.  Sentiment:\n",
              "Negative  Review: The cinematography was stunning and the performances were outstanding. Sentiment:\n",
              "Positive   Review: I found the movie to be pretty average - not great but not awful either.\n",
              "Sentiment: Neutral  Review: I absolutely loved this film! It kept me on the edge of my seat the\n",
              "whole time. Sentiment: \" Output: \"Positive\"  Chain of Thought Prompting: Definition: Encouraging the\n",
              "model to break down its reasoning into a series of steps, explaining its thought process, before\n",
              "providing the final answer. This allows the model to perform more complex, multi-step reasoning.\n",
              "Example: Input: \"Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3\n",
              "tennis balls. How many tennis balls does he have now? Let's solve this step by step.\"  Output: \"Step\n",
              "1: Understand the given information - Roger starts with 5 tennis balls - He buys 2 cans of tennis\n",
              "balls - Each can contains 3 tennis balls  Step 2: Calculate the number of tennis balls in the cans\n",
              "Roger bought - Roger bought 2 cans - Each can has 3 tennis balls  - So the total number of balls in\n",
              "the cans is: 2 cans × 3 balls per can = 6 balls  Step 3: Add the number of balls Roger started with\n",
              "to the number of balls in the cans - Roger started with 5 balls - The cans contain 6 balls - So the\n",
              "total number of balls Roger has now is: 5 starting balls + 6 balls from cans = 11 balls  Therefore,\n",
              "Roger now has 11 tennis balls in total.\"  Summary of Key Differences: | Technique | Input Provided |\n",
              "Reasoning Process | Complexity of Tasks |\n",
              "|-----------|----------------|-------------------|---------------------| | Zero-Shot | Task input\n",
              "only | None | Simple | | Few-Shot | Task input + few examples | Learns from examples | Moderate |  |\n",
              "Chain of Thought | Task input + prompt to explain steps | Breaks down reasoning into steps |\n",
              "Complex, multi-step |  In summary, zero-shot uses only the task input, few-shot learns from a small\n",
              "number of examples, and chain of thought breaks down the reasoning into explicit steps to tackle\n",
              "more complex problems."
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "render(chain_of_thought_response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pkh9Z_kRvKYo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}